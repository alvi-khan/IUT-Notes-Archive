{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b05ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:07.517843Z",
     "iopub.status.busy": "2021-12-07T14:22:07.516685Z",
     "iopub.status.idle": "2021-12-07T14:22:09.389728Z",
     "shell.execute_reply": "2021-12-07T14:22:09.388983Z",
     "shell.execute_reply.started": "2021-12-07T13:17:16.522574Z"
    },
    "papermill": {
     "duration": 1.948899,
     "end_time": "2021-12-07T14:22:09.389908",
     "exception": false,
     "start_time": "2021-12-07T14:22:07.441009",
     "status": "completed"
    },
    "tags": [],
    "id": "05b05ab9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "import re \n",
    "import os \n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f93fc3",
   "metadata": {
    "papermill": {
     "duration": 0.068504,
     "end_time": "2021-12-07T14:22:09.667817",
     "exception": false,
     "start_time": "2021-12-07T14:22:09.599313",
     "status": "completed"
    },
    "tags": [],
    "id": "c9f93fc3"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction</h1><a id = \"1\" ></a>\n",
    "\n",
    "\n",
    "In Natural Language Processing (NLP) the conversion of raw-text to numerical form is called <b>Text Representation</b> and believe me this step is one of the most important steps in the NLP pipeline as if we feed in poor features in ML Model, we will get poor results. In computer science, this is often called “garbage in, garbage out.”\n",
    "\n",
    "<b>I observed in NLP feeding a good text representation to an ordinary algorithm will get you much farther compared to applying a topnotch algorithm to an ordinary text representation.</b>\n",
    "\n",
    "In this notebook, I will discuss various text-representation schemes with their advantages and disadvantages so that you can choose one of the schemes which suit your task most. Our main objective is to transform a given text into numerical form so that it can be fed\n",
    "into NLP and ML algorithms.\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0301.png)\n",
    "\n",
    "In this notebook, the focus will be on the dotted box in the figure \n",
    "\n",
    "\n",
    "here write a para on the flow of the notebook later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a1cb0",
   "metadata": {
    "papermill": {
     "duration": 0.068209,
     "end_time": "2021-12-07T14:22:09.804899",
     "exception": false,
     "start_time": "2021-12-07T14:22:09.736690",
     "status": "completed"
    },
    "tags": [],
    "id": "c88a1cb0"
   },
   "source": [
    "But before moving on to the Text representation step first we have to get a cleaned dataset which then has to be preprocessed. In this notebook, I will be using only a few basic steps to preprocess the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732078de",
   "metadata": {
    "papermill": {
     "duration": 0.069141,
     "end_time": "2021-12-07T14:22:09.943204",
     "exception": false,
     "start_time": "2021-12-07T14:22:09.874063",
     "status": "completed"
    },
    "tags": [],
    "id": "732078de"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Load a Clean Dataset</h1><a id = \"2\" ></a>\n",
    "\n",
    "Kaggle Datasets is one of the best sources to get a clean dataset for this notebook I will be using [Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip /content/Tweets.csv.zip"
   ],
   "metadata": {
    "id": "HuBxA1dsVTus"
   },
   "id": "HuBxA1dsVTus",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca1117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.086920Z",
     "iopub.status.busy": "2021-12-07T14:22:10.086133Z",
     "iopub.status.idle": "2021-12-07T14:22:10.245386Z",
     "shell.execute_reply": "2021-12-07T14:22:10.244778Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.517214Z"
    },
    "papermill": {
     "duration": 0.233149,
     "end_time": "2021-12-07T14:22:10.245531",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.012382",
     "status": "completed"
    },
    "tags": [],
    "id": "1cca1117"
   },
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('./Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61d2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.387459Z",
     "iopub.status.busy": "2021-12-07T14:22:10.386765Z",
     "iopub.status.idle": "2021-12-07T14:22:10.413495Z",
     "shell.execute_reply": "2021-12-07T14:22:10.414124Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.663265Z"
    },
    "papermill": {
     "duration": 0.10019,
     "end_time": "2021-12-07T14:22:10.414298",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.314108",
     "status": "completed"
    },
    "tags": [],
    "id": "7e61d2cd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "outputId": "fe840b8f-2b1d-4285-97d8-0f997d5019dd"
   },
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13b864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.558837Z",
     "iopub.status.busy": "2021-12-07T14:22:10.558114Z",
     "iopub.status.idle": "2021-12-07T14:22:10.596904Z",
     "shell.execute_reply": "2021-12-07T14:22:10.597432Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.694501Z"
    },
    "papermill": {
     "duration": 0.113262,
     "end_time": "2021-12-07T14:22:10.597621",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.484359",
     "status": "completed"
    },
    "tags": [],
    "id": "2c13b864",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e269e97-0be6-4264-a049-8d41856cd756"
   },
   "outputs": [],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42104a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:10.740471Z",
     "iopub.status.busy": "2021-12-07T14:22:10.739435Z",
     "iopub.status.idle": "2021-12-07T14:22:10.972813Z",
     "shell.execute_reply": "2021-12-07T14:22:10.973327Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.735777Z"
    },
    "papermill": {
     "duration": 0.306738,
     "end_time": "2021-12-07T14:22:10.973515",
     "exception": false,
     "start_time": "2021-12-07T14:22:10.666777",
     "status": "completed"
    },
    "tags": [],
    "id": "8a42104a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "outputId": "366ccc91-ce64-4be5-ca5c-74614a16fa0b"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x = \"airline_sentiment\", data = clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e4b1e",
   "metadata": {
    "papermill": {
     "duration": 0.072146,
     "end_time": "2021-12-07T14:22:11.116201",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.044055",
     "status": "completed"
    },
    "tags": [],
    "id": "849e4b1e"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Basic Text Pre-Processing</h1><a id = \"3\" ></a>\n",
    "\n",
    "Text preprocessing steps include a few essential tasks to further clean the available text data. It includes tasks like:-\n",
    "\n",
    "**1. Stop-Word Removal** : In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n",
    "\n",
    "**2. Lower Casing** : Convert all words into the lower case because the upper or lower case may not make a difference for the problem.\n",
    "And we are reducing vocabulary size by doing so. \n",
    "\n",
    "**3. Stemming** : Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., “walk” and “walking” are both reduced to “walk”).\n",
    "\n",
    "**4. Tokenization** : NLP software typically analyzes text by breaking it up into words (tokens) and sentences.\n",
    "\n",
    "Pre-processing of the text is not the main objective of this notebook that's why I am just covering a few basic steps in a brief\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04744cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:11.266128Z",
     "iopub.status.busy": "2021-12-07T14:22:11.265361Z",
     "iopub.status.idle": "2021-12-07T14:22:11.267682Z",
     "shell.execute_reply": "2021-12-07T14:22:11.268298Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.964893Z"
    },
    "papermill": {
     "duration": 0.080846,
     "end_time": "2021-12-07T14:22:11.268478",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.187632",
     "status": "completed"
    },
    "tags": [],
    "id": "04744cce"
   },
   "outputs": [],
   "source": [
    "# First of all let's drop the columns which we don't required\n",
    "\n",
    "waste_col = ['tweet_id', 'airline_sentiment_confidence',\n",
    "       'negativereason', 'negativereason_confidence', 'airline',\n",
    "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
    "       'retweet_count', 'tweet_coord', 'tweet_created',\n",
    "       'tweet_location', 'user_timezone']\n",
    "\n",
    "data = clean_data.drop(waste_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae080b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:11.418865Z",
     "iopub.status.busy": "2021-12-07T14:22:11.418160Z",
     "iopub.status.idle": "2021-12-07T14:22:11.421007Z",
     "shell.execute_reply": "2021-12-07T14:22:11.421498Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.972057Z"
    },
    "papermill": {
     "duration": 0.082756,
     "end_time": "2021-12-07T14:22:11.421681",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.338925",
     "status": "completed"
    },
    "tags": [],
    "id": "76ae080b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "08e2765b-79d1-4bed-ea30-e5726ae38325"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76460d5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:11.569737Z",
     "iopub.status.busy": "2021-12-07T14:22:11.568997Z",
     "iopub.status.idle": "2021-12-07T14:22:11.571009Z",
     "shell.execute_reply": "2021-12-07T14:22:11.571526Z",
     "shell.execute_reply.started": "2021-12-07T13:17:18.992147Z"
    },
    "papermill": {
     "duration": 0.079454,
     "end_time": "2021-12-07T14:22:11.571716",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.492262",
     "status": "completed"
    },
    "tags": [],
    "id": "76460d5b"
   },
   "outputs": [],
   "source": [
    "def sentiment(x):\n",
    "    if x == 'positive':\n",
    "        return 2\n",
    "    elif x == 'negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4f92b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:11.717874Z",
     "iopub.status.busy": "2021-12-07T14:22:11.716842Z",
     "iopub.status.idle": "2021-12-07T14:22:11.852609Z",
     "shell.execute_reply": "2021-12-07T14:22:11.853330Z",
     "shell.execute_reply.started": "2021-12-07T13:17:19.000278Z"
    },
    "papermill": {
     "duration": 0.210989,
     "end_time": "2021-12-07T14:22:11.853570",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.642581",
     "status": "completed"
    },
    "tags": [],
    "id": "bff4f92b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4a1f43ed-9ba0-4000-92f3-89646cac3c80"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "# As this dataset is fetched from twitter so it has lots of people tag in tweets\n",
    "# we will remove them \n",
    "tags = r\"@\\w*\"\n",
    "\n",
    "\n",
    "def preprocess_text(sentence, stem = False):\n",
    "    \n",
    "    sentence = [re.sub(tags, \"\", sentence)]\n",
    "    text = []\n",
    "    for word in sentence:\n",
    "        \n",
    "        if word not in stopwords:\n",
    "            \n",
    "            if stem:\n",
    "                text.append(stemmer.stem(word).lower())\n",
    "            else:\n",
    "                text.append(word.lower())\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9e697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:12.004808Z",
     "iopub.status.busy": "2021-12-07T14:22:12.004036Z",
     "iopub.status.idle": "2021-12-07T14:22:12.008748Z",
     "shell.execute_reply": "2021-12-07T14:22:12.009714Z",
     "shell.execute_reply.started": "2021-12-07T13:17:19.103723Z"
    },
    "papermill": {
     "duration": 0.082926,
     "end_time": "2021-12-07T14:22:12.010037",
     "exception": false,
     "start_time": "2021-12-07T14:22:11.927111",
     "status": "completed"
    },
    "tags": [],
    "id": "78a9e697",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6ef6d793-63ce-4a89-f87f-20527abddea6"
   },
   "outputs": [],
   "source": [
    "print(f\"Orignal Text ----- {data.text[11]}\")\n",
    "print()\n",
    "print(f\"Preprocessed Text ----- {preprocess_text(data.text[11])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d5248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:12.161539Z",
     "iopub.status.busy": "2021-12-07T14:22:12.159924Z",
     "iopub.status.idle": "2021-12-07T14:22:12.371282Z",
     "shell.execute_reply": "2021-12-07T14:22:12.371809Z",
     "shell.execute_reply.started": "2021-12-07T13:17:19.112119Z"
    },
    "papermill": {
     "duration": 0.286869,
     "end_time": "2021-12-07T14:22:12.372021",
     "exception": false,
     "start_time": "2021-12-07T14:22:12.085152",
     "status": "completed"
    },
    "tags": [],
    "id": "805d5248",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "f1035825-58ca-403a-d1f8-5deddf9b6238"
   },
   "outputs": [],
   "source": [
    "data.text = data.text.map(preprocess_text)\n",
    "data['target'] = data.airline_sentiment.map(sentiment)\n",
    "data = data.drop(['airline_sentiment'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d801a4c",
   "metadata": {
    "papermill": {
     "duration": 0.07431,
     "end_time": "2021-12-07T14:22:12.520458",
     "exception": false,
     "start_time": "2021-12-07T14:22:12.446148",
     "status": "completed"
    },
    "tags": [],
    "id": "7d801a4c"
   },
   "source": [
    "Now we have preprocessed textual data so now we can proceed further in this notebook and discuss various text representation approaches in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9edb0b",
   "metadata": {
    "papermill": {
     "duration": 0.072323,
     "end_time": "2021-12-07T14:22:12.670810",
     "exception": false,
     "start_time": "2021-12-07T14:22:12.598487",
     "status": "completed"
    },
    "tags": [],
    "id": "5f9edb0b"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">One-Hot Encoding</h1><a id = \"4\" ></a>\n",
    "\n",
    "\n",
    "In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID (wid) that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V dimensional binary vector of 0s and 1s. This is done via a |V| dimension vector filled with all 0s barring the index, where index = wid. At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation.\n",
    "\n",
    "Consider an Example \n",
    "\n",
    "![](https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b46ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:12.818949Z",
     "iopub.status.busy": "2021-12-07T14:22:12.818244Z",
     "iopub.status.idle": "2021-12-07T14:22:12.821795Z",
     "shell.execute_reply": "2021-12-07T14:22:12.822317Z",
     "shell.execute_reply.started": "2021-12-07T13:17:19.468086Z"
    },
    "papermill": {
     "duration": 0.079496,
     "end_time": "2021-12-07T14:22:12.822488",
     "exception": false,
     "start_time": "2021-12-07T14:22:12.742992",
     "status": "completed"
    },
    "tags": [],
    "id": "09b46ee1"
   },
   "outputs": [],
   "source": [
    "#this is an example vocabulary just to make concept clear\n",
    "sample_vocab = ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'run', 'green', 'tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c360d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:12.970010Z",
     "iopub.status.busy": "2021-12-07T14:22:12.969337Z",
     "iopub.status.idle": "2021-12-07T14:22:17.704052Z",
     "shell.execute_reply": "2021-12-07T14:22:17.703010Z",
     "shell.execute_reply.started": "2021-12-07T13:17:19.474194Z"
    },
    "papermill": {
     "duration": 4.809595,
     "end_time": "2021-12-07T14:22:17.704275",
     "exception": false,
     "start_time": "2021-12-07T14:22:12.894680",
     "status": "completed"
    },
    "tags": [],
    "id": "d5c360d5"
   },
   "outputs": [],
   "source": [
    "# vocabulary of words present in dataset\n",
    "data_vocab = []\n",
    "for text in data.text:\n",
    "    for word in text:\n",
    "        if word not in data_vocab:\n",
    "            data_vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6920feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:17.871363Z",
     "iopub.status.busy": "2021-12-07T14:22:17.870435Z",
     "iopub.status.idle": "2021-12-07T14:22:17.875200Z",
     "shell.execute_reply": "2021-12-07T14:22:17.875690Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.211031Z"
    },
    "papermill": {
     "duration": 0.08584,
     "end_time": "2021-12-07T14:22:17.875860",
     "exception": false,
     "start_time": "2021-12-07T14:22:17.790020",
     "status": "completed"
    },
    "tags": [],
    "id": "e6920feb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4cc4ec5a-0bc8-492f-ffec-bd64fa9b5bfd"
   },
   "outputs": [],
   "source": [
    "#function to return one-hot representation of passed text\n",
    "def get_onehot_representation(text, vocab = data_vocab):\n",
    "    onehot_encoded = []\n",
    "    for word in text:\n",
    "        temp = [0]*len(vocab)\n",
    "        temp[vocab.index(word)-1] = 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded\n",
    "\n",
    "print(\"One Hot Representation for sentence \\\"the cat sat on the mat\\\" :\")\n",
    "get_onehot_representation(['the', 'cat', 'sat', 'on', 'the', 'mat'], sample_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6f31d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:18.027795Z",
     "iopub.status.busy": "2021-12-07T14:22:18.027074Z",
     "iopub.status.idle": "2021-12-07T14:22:18.030188Z",
     "shell.execute_reply": "2021-12-07T14:22:18.030685Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.224048Z"
    },
    "papermill": {
     "duration": 0.082567,
     "end_time": "2021-12-07T14:22:18.030870",
     "exception": false,
     "start_time": "2021-12-07T14:22:17.948303",
     "status": "completed"
    },
    "tags": [],
    "id": "f3c6f31d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6488ced3-0d25-4fe8-c670-79a68f1451df"
   },
   "outputs": [],
   "source": [
    "print(f'Length of Vocabulary : {len(data_vocab)}')\n",
    "print(f'Sample of Vocabulary : {data_vocab[302 : 312]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e8ab0",
   "metadata": {
    "papermill": {
     "duration": 0.07211,
     "end_time": "2021-12-07T14:22:18.175781",
     "exception": false,
     "start_time": "2021-12-07T14:22:18.103671",
     "status": "completed"
    },
    "tags": [],
    "id": "5a9e8ab0"
   },
   "source": [
    "We have 14276 different words in a given dataset thus this implies each word representation for one-hot encoding schema will be of 14276-dimensional vector mark that this much big representation is just for a single word if we consider the representation of a sentence which consist of let say 20 words in it then it will be represented with (20,14276) sized matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0d3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:18.371550Z",
     "iopub.status.busy": "2021-12-07T14:22:18.370111Z",
     "iopub.status.idle": "2021-12-07T14:22:18.375456Z",
     "shell.execute_reply": "2021-12-07T14:22:18.375914Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.240142Z"
    },
    "papermill": {
     "duration": 0.12788,
     "end_time": "2021-12-07T14:22:18.376127",
     "exception": false,
     "start_time": "2021-12-07T14:22:18.248247",
     "status": "completed"
    },
    "tags": [],
    "id": "6ae0d3ef",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c3f81972-d7be-41a2-8d2b-d56b464e89df"
   },
   "outputs": [],
   "source": [
    "sample_one_hot_rep = get_onehot_representation(data.text[7], data_vocab)\n",
    "print(f\"Shapes of a single sentence : {np.array(sample_one_hot_rep).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00e7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:18.526924Z",
     "iopub.status.busy": "2021-12-07T14:22:18.525529Z",
     "iopub.status.idle": "2021-12-07T14:22:18.529338Z",
     "shell.execute_reply": "2021-12-07T14:22:18.528715Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.298219Z"
    },
    "papermill": {
     "duration": 0.08036,
     "end_time": "2021-12-07T14:22:18.529486",
     "exception": false,
     "start_time": "2021-12-07T14:22:18.449126",
     "status": "completed"
    },
    "tags": [],
    "id": "9d00e7b1"
   },
   "outputs": [],
   "source": [
    "#one-hot representation for dataset sentences \n",
    "\n",
    "# data.loc[:, 'one_hot_rep'] = data.loc[:, 'text'].map(get_onehot_representation)\n",
    "\n",
    "#if you run this cell it will give you a memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0130d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:18.686726Z",
     "iopub.status.busy": "2021-12-07T14:22:18.685615Z",
     "iopub.status.idle": "2021-12-07T14:22:18.690158Z",
     "shell.execute_reply": "2021-12-07T14:22:18.689593Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.311020Z"
    },
    "papermill": {
     "duration": 0.087987,
     "end_time": "2021-12-07T14:22:18.690306",
     "exception": false,
     "start_time": "2021-12-07T14:22:18.602319",
     "status": "completed"
    },
    "tags": [],
    "id": "ea0130d6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "4bdfe2c4-57cc-4b90-ec7a-219b13260a85"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c092a14",
   "metadata": {
    "papermill": {
     "duration": 0.073998,
     "end_time": "2021-12-07T14:22:18.838243",
     "exception": false,
     "start_time": "2021-12-07T14:22:18.764245",
     "status": "completed"
    },
    "tags": [],
    "id": "6c092a14"
   },
   "source": [
    "One-hot encoding is intuitive to understand and straightforward to implement. However, it has lots of disadvantages listed below\n",
    "\n",
    "1. The size of a one-hot vector is directly proportional to the size of the vocabulary and if we consider a real-world vocabulary size it may be in millions so we can not represent a single word with a million-dimensional vector. \n",
    "\n",
    "2. One-hot representation does not give a fixed-length representation for text, i.e., the sentence with 32 words in it and 40 words in it has variable length representation. But for most learning algorithms, we need the feature vectors to be of the same length.\n",
    "\n",
    "3. One-Hot representation gives each word the same weight whether that word is important for the task or not.\n",
    "\n",
    "4. One-Hot representation does not represent the meaning of the word in a proper numerical manner as embedding vectors do. Consider an example word read, reading should have similar real-valued vector representation but in this case, they have different representations. \n",
    "\n",
    "5. Let say we train the model on some article and get the vocabulary of size 10000 but what if we use this vocabulary on that text which contains words that are not present in learned vocabulary. This is Known as **Out Of Vocabulary (OOV)** problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b43c7",
   "metadata": {
    "papermill": {
     "duration": 0.07376,
     "end_time": "2021-12-07T14:22:18.987246",
     "exception": false,
     "start_time": "2021-12-07T14:22:18.913486",
     "status": "completed"
    },
    "tags": [],
    "id": "3f2b43c7"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of words</h1><a id = \"5\" ></a>\n",
    "\n",
    "Bag of words (BoW) is a classical text representation technique that has been used commonly in NLP, especially in text classification problems. The key idea behind it is as follows: represent the text under consideration as a bag (collection) of words while ignoring the order and context.\n",
    "\n",
    "Similar to one-hot encoding, BoW maps words to unique integer IDs between 1 and |V|. Each document in the corpus is then converted into a vector of |V| dimensions were in the ith component of the vector, i = wid, is simply the number of times the word w occurs in the document, i.e., we simply score each word in V by their occurrence count in the document.\n",
    "\n",
    "Consider an example:\n",
    "\n",
    "let say we have a vocabulary **V consisting of words --> {the, cat, sat, in, hat, with}** then the bag of word representation of a few sentences will be given as \n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*3IACMnNpwVlCl8kSTJocPA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba511c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:19.277629Z",
     "iopub.status.busy": "2021-12-07T14:22:19.269750Z",
     "iopub.status.idle": "2021-12-07T14:22:19.281592Z",
     "shell.execute_reply": "2021-12-07T14:22:19.280946Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.331174Z"
    },
    "papermill": {
     "duration": 0.221195,
     "end_time": "2021-12-07T14:22:19.281750",
     "exception": false,
     "start_time": "2021-12-07T14:22:19.060555",
     "status": "completed"
    },
    "tags": [],
    "id": "ba511c74",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ec70f21-fbd1-4190-ca61-a6b1c22f9edc"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_bow = CountVectorizer()\n",
    "\n",
    "# sample_corpus = [['the', 'cat', 'sat'], \n",
    "#                  ['the', 'cat', 'sat', 'in', 'the', 'hat'],\n",
    "#                  ['the', 'cat', 'with', 'the', 'hat']]\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "\n",
    "sample_bow.fit(sample_corpus)\n",
    "\n",
    "def get_bow_representation(text):\n",
    "        return sample_bow.transform(text)\n",
    "    \n",
    "print(f\"Vocabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\n",
    "print(\"\\nBag of word Representation of sentence 'the cat cat sat in the hat'\")\n",
    "print(get_bow_representation([\"the cat cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d12d8",
   "metadata": {
    "papermill": {
     "duration": 0.073638,
     "end_time": "2021-12-07T14:22:19.431023",
     "exception": false,
     "start_time": "2021-12-07T14:22:19.357385",
     "status": "completed"
    },
    "tags": [],
    "id": "4e8d12d8"
   },
   "source": [
    "Sometimes, we don’t care about the frequency of occurrence of words in the text and we only want to represent whether a word exists in the text or not. In such cases, we just initialize CountVectorizer with the binary=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d3f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:19.587910Z",
     "iopub.status.busy": "2021-12-07T14:22:19.586810Z",
     "iopub.status.idle": "2021-12-07T14:22:19.592132Z",
     "shell.execute_reply": "2021-12-07T14:22:19.591593Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.346771Z"
    },
    "papermill": {
     "duration": 0.086886,
     "end_time": "2021-12-07T14:22:19.592276",
     "exception": false,
     "start_time": "2021-12-07T14:22:19.505390",
     "status": "completed"
    },
    "tags": [],
    "id": "e84d3f35",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90d75530-0e38-44fc-db29-f0036b588473"
   },
   "outputs": [],
   "source": [
    "sample_bow = CountVectorizer(binary = True)\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "\n",
    "sample_bow.fit(sample_corpus)\n",
    "\n",
    "def get_bow_representation(text):\n",
    "        return sample_bow.transform(text)\n",
    "    \n",
    "print(f\"Vacabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\n",
    "print(\"\\nBag of word Representation of sentence 'the the the the cat cat sat in the hat'\")\n",
    "print(get_bow_representation([\"the the the the cat cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1823875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:19.745364Z",
     "iopub.status.busy": "2021-12-07T14:22:19.744678Z",
     "iopub.status.idle": "2021-12-07T14:22:20.196150Z",
     "shell.execute_reply": "2021-12-07T14:22:20.196698Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.357825Z"
    },
    "papermill": {
     "duration": 0.530116,
     "end_time": "2021-12-07T14:22:20.196874",
     "exception": false,
     "start_time": "2021-12-07T14:22:19.666758",
     "status": "completed"
    },
    "tags": [],
    "id": "f1823875"
   },
   "outputs": [],
   "source": [
    "# generate bag of word representation for given dataset\n",
    "\n",
    "bow = CountVectorizer()\n",
    "bow_rep = bow.fit_transform(data.loc[:, 'text'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2bd97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:20.349360Z",
     "iopub.status.busy": "2021-12-07T14:22:20.348686Z",
     "iopub.status.idle": "2021-12-07T14:22:20.351571Z",
     "shell.execute_reply": "2021-12-07T14:22:20.352106Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.836071Z"
    },
    "papermill": {
     "duration": 0.081172,
     "end_time": "2021-12-07T14:22:20.352304",
     "exception": false,
     "start_time": "2021-12-07T14:22:20.271132",
     "status": "completed"
    },
    "tags": [],
    "id": "aed2bd97"
   },
   "outputs": [],
   "source": [
    "# intrested one can see vocabulary of given corpus by uncommenting below code line\n",
    "\n",
    "# bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7383a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:20.505496Z",
     "iopub.status.busy": "2021-12-07T14:22:20.504824Z",
     "iopub.status.idle": "2021-12-07T14:22:20.952478Z",
     "shell.execute_reply": "2021-12-07T14:22:20.953221Z",
     "shell.execute_reply.started": "2021-12-07T13:17:24.841865Z"
    },
    "papermill": {
     "duration": 0.526904,
     "end_time": "2021-12-07T14:22:20.953452",
     "exception": false,
     "start_time": "2021-12-07T14:22:20.426548",
     "status": "completed"
    },
    "tags": [],
    "id": "dc7383a5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6fa2bcb0-417d-4dac-e130-7f586b857155"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of Bag of word representaion matrix : {bow_rep.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac8045",
   "metadata": {
    "papermill": {
     "duration": 0.074945,
     "end_time": "2021-12-07T14:22:21.104849",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.029904",
     "status": "completed"
    },
    "tags": [],
    "id": "2aac8045"
   },
   "source": [
    "As we know One-hot representation does not give a fixed-length representation for text but from the output of the above cell, we can interpret that bag of the word has a fixed-length vector representation (14238 dimensional) for each word but it is too big again in this case also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0e573",
   "metadata": {
    "papermill": {
     "duration": 0.073905,
     "end_time": "2021-12-07T14:22:21.253882",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.179977",
     "status": "completed"
    },
    "tags": [],
    "id": "46e0e573"
   },
   "source": [
    "**Advantages of this Bag of words(BoW) encoding** :\n",
    "\n",
    "1. Like one-hot encoding, BoW is fairly simple to understand and implement.\n",
    "\n",
    "2. With this representation, documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words.\n",
    "\n",
    "    Consider an example Where \n",
    "\n",
    "    S1 = \"cat on the mat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n",
    "    S2 = \"mat on the cat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n",
    "    S3 = \"dog in the mat\" --> BoW Representation --> {0 1 0 1 1 1 0} <br>\n",
    "\n",
    "    The distance between S1 and S2 is 0 as compared to the distance between S1 and S3, which is 2. Thus, the vector space resulting from the BoW scheme captures the semantic similarity of documents. So if two documents have a similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
    "\n",
    "3. We have a fixed-length encoding for any sentence of arbitrary length.\n",
    "\n",
    "**Disadvantages of this Bag of words(BoW) encoding** :\n",
    "\n",
    "1. The size of the vector increases with the size of the vocabulary as in our case it is 14238 dimensional. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n",
    "\n",
    "2. It does not capture the similarity between different words that mean the same thing. Say we have three documents: “walk”, “walked”, and “walking”. BoW vectors of all three documents will be equally apart.\n",
    "\n",
    "3. This representation does not have any way to handle **out of vocabulary (OOV)** words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n",
    "\n",
    "4. As the name indicates, it is a “bag” of words—word order information is lost in this representation. Both S1 and S2 will have the same representation in this scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49b0ec",
   "metadata": {
    "papermill": {
     "duration": 0.074554,
     "end_time": "2021-12-07T14:22:21.402773",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.328219",
     "status": "completed"
    },
    "tags": [],
    "id": "be49b0ec"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of N-Grams</h1><a id = \"6\" ></a>\n",
    "\n",
    "All the representation schemes we’ve seen so far treat words as independent units. There is no notion of phrases or word order. The bag-of-n-grams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an n-gram.\n",
    "\n",
    "**One can simply say Bag of words (BoW) is a special case of the Bag of n-grams having n = 1.**\n",
    "\n",
    "The corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the corpus is represented by a vector of length |V|. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.\n",
    "\n",
    "Consider an Example: \n",
    "\n",
    "![](https://i.stack.imgur.com/8ARA1.png)\n",
    "\n",
    "\n",
    "The following code cell shows an example of a BoN representation considering 1–3 n-gram word features to represent the corpus that we’ve used so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2dbec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:21.559902Z",
     "iopub.status.busy": "2021-12-07T14:22:21.559223Z",
     "iopub.status.idle": "2021-12-07T14:22:21.564233Z",
     "shell.execute_reply": "2021-12-07T14:22:21.564824Z",
     "shell.execute_reply.started": "2021-12-07T13:17:25.300886Z"
    },
    "papermill": {
     "duration": 0.087884,
     "end_time": "2021-12-07T14:22:21.565015",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.477131",
     "status": "completed"
    },
    "tags": [],
    "id": "dac2dbec",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7817acf5-500c-4d02-ed39-b6d549a4f2dd"
   },
   "outputs": [],
   "source": [
    "# Bag of 1-gram (unigram)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_boN = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "\n",
    "sample_boN.fit(sample_corpus)\n",
    "\n",
    "def get_boN_representation(text):\n",
    "        return sample_boN.transform(text)\n",
    "    \n",
    "print(f\"Unigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\n",
    "print(\"\\nBag of 1-gram (unigram) Representation of sentence 'the cat cat sat in the hat'\")\n",
    "print(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72339ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:21.718436Z",
     "iopub.status.busy": "2021-12-07T14:22:21.717771Z",
     "iopub.status.idle": "2021-12-07T14:22:21.725744Z",
     "shell.execute_reply": "2021-12-07T14:22:21.726317Z",
     "shell.execute_reply.started": "2021-12-07T13:17:25.311601Z"
    },
    "papermill": {
     "duration": 0.086755,
     "end_time": "2021-12-07T14:22:21.726496",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.639741",
     "status": "completed"
    },
    "tags": [],
    "id": "72339ae4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ec5ea93f-dc5b-40c0-c570-cfb8c2e0b203"
   },
   "outputs": [],
   "source": [
    "# Bag of 2-gram (bigram)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_boN = CountVectorizer(ngram_range = (2, 2))\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "\n",
    "sample_boN.fit(sample_corpus)\n",
    "\n",
    "def get_boN_representation(text):\n",
    "        return sample_boN.transform(text)\n",
    "    \n",
    "print(f\"Bigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\n",
    "print(\"\\nBag of 2-gram (bigram) Representation of sentence 'the cat cat sat in the hat'\")\n",
    "print(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0129e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:21.885259Z",
     "iopub.status.busy": "2021-12-07T14:22:21.884061Z",
     "iopub.status.idle": "2021-12-07T14:22:21.889664Z",
     "shell.execute_reply": "2021-12-07T14:22:21.890402Z",
     "shell.execute_reply.started": "2021-12-07T13:17:25.325155Z"
    },
    "papermill": {
     "duration": 0.088166,
     "end_time": "2021-12-07T14:22:21.890627",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.802461",
     "status": "completed"
    },
    "tags": [],
    "id": "c0d0129e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51cae62d-b8a9-4b53-f2e5-60f29e7a314c"
   },
   "outputs": [],
   "source": [
    "# Bag of 3-gram (trigram)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_boN = CountVectorizer(ngram_range = (3, 3))\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "\n",
    "sample_boN.fit(sample_corpus)\n",
    "\n",
    "def get_boN_representation(text):\n",
    "        return sample_boN.transform(text)\n",
    "    \n",
    "print(f\"Trigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\n",
    "print(\"\\nBag of 3-gram (trigram) Representation of sentence 'the cat cat sat in the hat'\")\n",
    "print(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cafa10d",
   "metadata": {
    "papermill": {
     "duration": 0.075393,
     "end_time": "2021-12-07T14:22:22.042863",
     "exception": false,
     "start_time": "2021-12-07T14:22:21.967470",
     "status": "completed"
    },
    "tags": [],
    "id": "6cafa10d"
   },
   "source": [
    "**Here are the main advantages and disadvantages of BoN Representation:**\n",
    "\n",
    "1. It captures some context and word-order information in the form of n-grams.\n",
    "\n",
    "2. Thus, the resulting vector space can capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.\n",
    "\n",
    "3. As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n",
    "\n",
    "4. It still provides no way to address the **out of vocabulary(OOV)** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4709",
   "metadata": {
    "papermill": {
     "duration": 0.07612,
     "end_time": "2021-12-07T14:22:22.195176",
     "exception": false,
     "start_time": "2021-12-07T14:22:22.119056",
     "status": "completed"
    },
    "tags": [],
    "id": "2c2d4709"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">TF-IDF</h1><a id = \"7\" ></a>\n",
    "\n",
    "\n",
    "In all the three approaches we’ve seen so far, all the words in the text are treated as equally important—there’s no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.\n",
    "\n",
    "The intuition behind TF-IDF is as follows: if a word w appears many times in a sentence S1 but does not occur much in the rest of the Sentences Sn in the corpus, then the word w must be of great importance to the Sentence S1. The importance of w should increase in proportion to its frequency in S1 (how many times that word occurs in sentence S1), but at the same time, its importance should decrease in proportion to the word’s frequency in other Sentence Sn in the corpus. **Mathematically, this is captured using two quantities: TF and IDF. The two are then multiplied to arrive at the TF-IDF score.**\n",
    "\n",
    "**TF (term frequency) measures how often a term or word occurs in a given document.**\n",
    "\n",
    "Mathematical Expression of TF\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)\n",
    "\n",
    "**IDF (inverse document frequency)** measures the importance of the term across a corpus. In computing TF, all terms are given equal importance (weightage). However, it’s a well-known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. IDF of a term t is calculated as follows:\n",
    "\n",
    "![](https://mungingdata.files.wordpress.com/2017/11/tfidf.png)\n",
    "\n",
    "The TF-IDF score is a product of these two terms. Thus, TF-IDF score = TF * IDF. Let’s consider an example.\n",
    "\n",
    "Sentence A = The Car is Driven on the Road <br>\n",
    "Sentence B = The Truck is Driven on the highway <br>\n",
    "\n",
    "Computation of TF-IDF scores are shown below\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/1*q3qYevXqQOjJf6Pwdlx8Mw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfeeec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:22:22.349367Z",
     "iopub.status.busy": "2021-12-07T14:22:22.348641Z",
     "iopub.status.idle": "2021-12-07T14:22:22.362144Z",
     "shell.execute_reply": "2021-12-07T14:22:22.362714Z",
     "shell.execute_reply.started": "2021-12-07T13:17:25.340031Z"
    },
    "papermill": {
     "duration": 0.092602,
     "end_time": "2021-12-07T14:22:22.362886",
     "exception": false,
     "start_time": "2021-12-07T14:22:22.270284",
     "status": "completed"
    },
    "tags": [],
    "id": "decfeeec",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8a749891-377d-43d9-df40-98128780345f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
    "tfidf_rep = tfidf.fit_transform(sample_corpus)\n",
    "print(f\"IDF Values for sample corpus : {tfidf.idf_}\")\n",
    "\n",
    "\n",
    "print(\"TF-IDF Representation for sentence 'the cat sat in the hat' :\") \n",
    "print(tfidf.transform([\"the cat sat in the hat\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58840a00",
   "metadata": {
    "papermill": {
     "duration": 0.076534,
     "end_time": "2021-12-07T14:22:22.515592",
     "exception": false,
     "start_time": "2021-12-07T14:22:22.439058",
     "status": "completed"
    },
    "tags": [],
    "id": "58840a00"
   },
   "source": [
    "Similar to BoW, we can use the TF-IDF vectors to calculate the similarity between two texts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is a commonly used representation in application scenarios such as information\n",
    "retrieval and text classification. However, even though TF-IDF is better than the vectorization methods we saw earlier in terms of capturing similarities between words, **it still suffers from the curse of high dimensionality.**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ],
   "metadata": {
    "id": "VWYCVKvfUH4u"
   },
   "id": "VWYCVKvfUH4u",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "y = data['target']"
   ],
   "metadata": {
    "id": "8wd3q1mYUJd8"
   },
   "id": "8wd3q1mYUJd8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "id": "SvkKOm_rUMZ8"
   },
   "id": "SvkKOm_rUMZ8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy of logistic regression classifier:', accuracy_score(y_test, y_pred_logreg))\n",
    "print('Confusion matrix of logistic regression classifier:')\n",
    "print(confusion_matrix(y_test, y_pred_logreg))"
   ],
   "metadata": {
    "id": "WRVOy0PIUN0U",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "57d5869f-ee81-49f5-e238-b0869858e162"
   },
   "id": "WRVOy0PIUN0U",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(80,),early_stopping=True, max_iter=50)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "print('Accuracy of MLP classifier:', accuracy_score(y_test, y_pred_mlp))\n",
    "print('Confusion matrix of MLP classifier:')\n",
    "print(confusion_matrix(y_test, y_pred_mlp))"
   ],
   "metadata": {
    "id": "mVnotVceUPqk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f2a74c48-6209-4939-b8de-5dfc86e07a4f"
   },
   "id": "mVnotVceUPqk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6c38a382",
   "metadata": {
    "papermill": {
     "duration": 0.076053,
     "end_time": "2021-12-07T14:22:22.668690",
     "exception": false,
     "start_time": "2021-12-07T14:22:22.592637",
     "status": "completed"
    },
    "tags": [],
    "id": "6c38a382"
   },
   "source": [
    "**Here are the main advantages and disadvantages of TF-IDF Representation:**\n",
    "\n",
    "1. Its Implementation is not that easy as compared to techniques discussed above \n",
    "2. We have a fixed-length encoding for any sentence of arbitrary length.\n",
    "3. The feature vectors are high-dimensional representations. The dimensionality increases with the size of the vocabulary.\n",
    "4. It did capture a bit of the semantics of the sentence. \n",
    "5. They too cannot handle OOV words.\n",
    "\n",
    "With this, we come to the end of basic vectorization approaches. Now, let’s start looking at distributed representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff66412",
   "metadata": {
    "papermill": {
     "duration": 0.076367,
     "end_time": "2021-12-07T14:22:22.821093",
     "exception": false,
     "start_time": "2021-12-07T14:22:22.744726",
     "status": "completed"
    },
    "tags": [],
    "id": "dff66412"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Word2vec Word Embeddings</h1><a id = \"8\" ></a>\n",
    "\n",
    "**Word Embeddings** : They are a real-valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real-valued vector \n",
    "\n",
    "Word2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n",
    "\n",
    "It is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.\n",
    "\n",
    "**Using Pre-trained word2vec word embeddings** <br>\n",
    "Training your own word embeddings is a pretty expensive process (in terms of both time and computing). Thankfully, for many scenarios, it’s not necessary to train your own embeddings Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings\n",
    "can be downloaded and used to get the vectors for the words you want.  \n",
    "\n",
    "Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few.\n",
    "\n",
    "Below code, cell demonstrates how to use pre-trained word2vec word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "api.info()"
   ],
   "metadata": {
    "id": "CM9JIvA-fg_p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "baa4e03a-09b7-43dd-ea13-e947f861fc53"
   },
   "id": "CM9JIvA-fg_p",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "Word2VecModel = api.load(\"word2vec-google-news-300\")"
   ],
   "metadata": {
    "id": "-olCDTLuavXr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "729d9ab8-934a-49be-9842-19a1fde2cb85"
   },
   "id": "-olCDTLuavXr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8bb37176",
   "metadata": {
    "papermill": {
     "duration": 0.07589,
     "end_time": "2021-12-07T14:23:39.298193",
     "exception": false,
     "start_time": "2021-12-07T14:23:39.222303",
     "status": "completed"
    },
    "tags": [],
    "id": "8bb37176"
   },
   "source": [
    "As we learned Word2vec do have a similar vector representation for words with the same meaning so let's check the similar words for the word \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a5264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:23:39.456724Z",
     "iopub.status.busy": "2021-12-07T14:23:39.456027Z",
     "iopub.status.idle": "2021-12-07T14:23:42.585042Z",
     "shell.execute_reply": "2021-12-07T14:23:42.586729Z",
     "shell.execute_reply.started": "2021-12-07T13:18:39.625000Z"
    },
    "papermill": {
     "duration": 3.212874,
     "end_time": "2021-12-07T14:23:42.587205",
     "exception": false,
     "start_time": "2021-12-07T14:23:39.374331",
     "status": "completed"
    },
    "tags": [],
    "id": "687a5264",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e8965b51-2d72-474f-f9b5-357677ef8e4c"
   },
   "outputs": [],
   "source": [
    "print(Word2VecModel.most_similar('good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5debc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:23:42.813344Z",
     "iopub.status.busy": "2021-12-07T14:23:42.812394Z",
     "iopub.status.idle": "2021-12-07T14:23:42.816473Z",
     "shell.execute_reply": "2021-12-07T14:23:42.817001Z",
     "shell.execute_reply.started": "2021-12-07T13:18:42.030295Z"
    },
    "papermill": {
     "duration": 0.142583,
     "end_time": "2021-12-07T14:23:42.817179",
     "exception": false,
     "start_time": "2021-12-07T14:23:42.674596",
     "status": "completed"
    },
    "tags": [],
    "id": "6c5debc1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "44839690-6120-4f66-ccbc-35e0828d9b5b"
   },
   "outputs": [],
   "source": [
    "print(Word2VecModel['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2887b",
   "metadata": {
    "papermill": {
     "duration": 0.07775,
     "end_time": "2021-12-07T14:23:42.972094",
     "exception": false,
     "start_time": "2021-12-07T14:23:42.894344",
     "status": "completed"
    },
    "tags": [],
    "id": "cdf2887b"
   },
   "source": [
    "In the output of the above code cell, we can see a 300-dimensional real-valued vector for the word \"good\".\n",
    "\n",
    "The above few cells were about using pre-trained word2vec representation now in upcoming cells we will focus on learning/Training our own word2vec representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958bcef",
   "metadata": {
    "papermill": {
     "duration": 0.076746,
     "end_time": "2021-12-07T14:23:43.126254",
     "exception": false,
     "start_time": "2021-12-07T14:23:43.049508",
     "status": "completed"
    },
    "tags": [],
    "id": "4958bcef"
   },
   "source": [
    "**Training our own embeddings**\n",
    "\n",
    "Now we’ll focus on training our own word embeddings. For this, we’ll look at two architectural variants that were proposed in the original Word2vec approach. The two variants are:\n",
    "\n",
    "1. Continuous bag of words (CBOW)\n",
    "2. SkipGram\n",
    "\n",
    "Both of these have a lot of similarities in many respects. \n",
    "\n",
    "Throughout this section, we’ll use the sentence “The quick brown fox jumps over the lazy dog” as our example text.\n",
    "\n",
    "**1. Continuous bag of words (CBOW)**\n",
    "\n",
    "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. Consider our example sentence we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word.\n",
    "<br><br>\n",
    "Now next task is to create a training sample of the form (X, Y) for this task where X will be context words and Y will be Center word. We define the value of context window = 2 in this case.<br> \n",
    "![](https://miro.medium.com/max/536/1*vZhxrBkCz-yN_rzZBqSKiA.png)\n",
    "\n",
    "<br><br>\n",
    "Now that we have the training data ready, let’s focus on the model. For this, we construct a shallow net (it’s shallow since it has a single hidden layer). We assume we want to learn D-dim word embeddings. Further, let V be the vocabulary of the text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4b64d",
   "metadata": {
    "papermill": {
     "duration": 0.078419,
     "end_time": "2021-12-07T14:23:43.280598",
     "exception": false,
     "start_time": "2021-12-07T14:23:43.202179",
     "status": "completed"
    },
    "tags": [],
    "id": "dfa4b64d"
   },
   "source": [
    "<img src=\"https://file.techscience.com/ueditor/files/TSP_CSSE_36-1/TSP_CSSE_14260/TSP_CSSE_14260/images/fig-1.png\" width=\"600\">\n",
    "\n",
    "For more information you can checkout https://lilianweng.github.io/posts/2017-10-15-word-embedding/\n",
    "<br><br>\n",
    "The objective is to learn an embedding matrix W|V| x d.To begin with, we initialize the matrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of the embedding. Let’s break down the shallow net in Figure layer by layer. In the input layer, indices of the words in context are used to fetch the corresponding rows from the embedding matrix W|V| x d. The vectors fetched are then added to get a single D-dim vector, and this is passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix W’d x |V|.. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5ab1b",
   "metadata": {
    "papermill": {
     "duration": 0.086327,
     "end_time": "2021-12-07T14:23:43.464620",
     "exception": false,
     "start_time": "2021-12-07T14:23:43.378293",
     "status": "completed"
    },
    "tags": [],
    "id": "41c5ab1b"
   },
   "source": [
    "**2. SkipGram** \n",
    "\n",
    "SkipGram is very similar to CBOW, with some minor changes. In Skip‐ Gram, the task is to predict the context words from the center word. For our toy corpus with context size 2, using the center word “jumps,” we try to predict every word in context—“brown,” “fox,” “over,” “the”.\n",
    "\n",
    "Now we will create a training sample of the form (X, Y) for this task where X will be the center word and Y will be Context words. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*jkxbwD55_8M3XBRb1bGm7A.png\" width=\"600\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://pub.mdpi-res.com/futureinternet/futureinternet-11-00114/article_deploy/html/images/futureinternet-11-00114-g001.png?1571521360\" width=\"600\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The shallow network used to train the SkipGram model, shown in the below Figure, is very similar to the network used for CBOW, with some minor changes. In the input layer, the index of the word in the target is used to fetch the corresponding row from the embedding matrix W|V| x d. The vectors fetched are then passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix W’d x |V|. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices W and W’ accordingly. At the end of the training, W is the embedding matrix we wanted to learn.\n",
    "\n",
    "For more information you can checkout https://lilianweng.github.io/posts/2017-10-15-word-embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696ba4c",
   "metadata": {
    "papermill": {
     "duration": 0.08588,
     "end_time": "2021-12-07T14:23:43.634447",
     "exception": false,
     "start_time": "2021-12-07T14:23:43.548567",
     "status": "completed"
    },
    "tags": [],
    "id": "0696ba4c"
   },
   "source": [
    "**Implementation** : \n",
    "\n",
    "One of the most commonly used implementations is with gensim. We have to choose several hyperparameters (i.e., the variables that need to be set before starting the training process). Let’s look at two examples.\n",
    "\n",
    "Dimensionality of the word vectors\n",
    "\n",
    "As the name indicates, this decides the space of the learned embeddings. While there is no ideal number, it’s common to construct word vectors with dimensions in the range of 50–500 and evaluate them on the task we’re using them for to choose the best option. In gensim we do this by setting the \"vector_size\" parameter to the size we want. \n",
    "\n",
    "Context window\n",
    "\n",
    "How long or short the context we look for to learn the vector representation is. In gensim we do this by setting the \"window\" parameter to the size we want.\n",
    "\n",
    "There are also other choices we make, such as whether to use CBOW or SkipGram to\n",
    "learn the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf09d92",
   "metadata": {
    "papermill": {
     "duration": 0.077271,
     "end_time": "2021-12-07T14:23:44.849036",
     "exception": false,
     "start_time": "2021-12-07T14:23:44.771765",
     "status": "completed"
    },
    "tags": [],
    "id": "ddf09d92"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Glove Word Embeddings</h1><a id = \"9\" ></a>\n",
    "\n",
    "GloVe Stands for Global Vectors for word representation is another word embedding technique that was developed as an open-source project at Stanford and was launched in 2014. Just to refresh, word vectors put words to a nice vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors. But keep in mind that there’s quite a bit of synergy between the GloVe and Word2vec. The gloVe can be used to find relations between words like synonyms, company-product relations, zip codes, and cities, etc.\n",
    "\n",
    "The question may arise Why do we need Glove if we have word2vec as a good word embedding technique Because Word2vec relies only on local information of language. That is, the semantics learned for a given word, are only affected by the surrounding words.\n",
    "\n",
    "For example, take the sentence,\n",
    "\n",
    "The cat sat on the mat\n",
    "\n",
    "If you use Word2vec, it wouldn’t capture information like,\n",
    "\n",
    "is “the” a special context of the words “cat” and “mat” ?\n",
    "\n",
    "or\n",
    "\n",
    "is “the” just a stopword?\n",
    "\n",
    "This can be suboptimal, especially in the eye of theoreticians.\n",
    "\n",
    "GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.\n",
    "\n",
    "![](https://miro.medium.com/max/434/1*QWcK8CIDs8kMkOwsOxvywA.png)\n",
    "\n",
    "The co-occurrence matrix for the sentence “the cat sat on the mat” with a window size of 1. As you probably noticed it is a symmetric matrix.\n",
    "\n",
    "For detailed knowledge about Glove word embedding, you can refer [This article](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57ec6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:23:45.011448Z",
     "iopub.status.busy": "2021-12-07T14:23:45.009576Z",
     "iopub.status.idle": "2021-12-07T14:25:34.924552Z",
     "shell.execute_reply": "2021-12-07T14:25:34.925132Z",
     "shell.execute_reply.started": "2021-12-07T13:18:42.327261Z"
    },
    "papermill": {
     "duration": 109.998348,
     "end_time": "2021-12-07T14:25:34.925334",
     "exception": false,
     "start_time": "2021-12-07T14:23:44.926986",
     "status": "completed"
    },
    "tags": [],
    "id": "cb57ec6c"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "Glove_model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:26:49.772334Z",
     "iopub.status.busy": "2021-12-07T14:26:49.771637Z",
     "iopub.status.idle": "2021-12-07T14:26:49.912017Z",
     "shell.execute_reply": "2021-12-07T14:26:49.912892Z",
     "shell.execute_reply.started": "2021-12-07T13:21:45.496829Z"
    },
    "papermill": {
     "duration": 0.224579,
     "end_time": "2021-12-07T14:26:49.913184",
     "exception": false,
     "start_time": "2021-12-07T14:26:49.688605",
     "status": "completed"
    },
    "tags": [],
    "id": "38a95065",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "452f0b29-bcdb-4953-8035-e6d20f997b6d"
   },
   "outputs": [],
   "source": [
    "print(\"Most similar words to word 'human' : \")\n",
    "Glove_model.most_similar('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d69b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:26:50.127328Z",
     "iopub.status.busy": "2021-12-07T14:26:50.126361Z",
     "iopub.status.idle": "2021-12-07T14:26:50.131030Z",
     "shell.execute_reply": "2021-12-07T14:26:50.131512Z",
     "shell.execute_reply.started": "2021-12-07T13:21:45.615577Z"
    },
    "papermill": {
     "duration": 0.093864,
     "end_time": "2021-12-07T14:26:50.131686",
     "exception": false,
     "start_time": "2021-12-07T14:26:50.037822",
     "status": "completed"
    },
    "tags": [],
    "id": "e85d69b2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "13e286c8-a614-4485-8c10-c3a4c1f1971f"
   },
   "outputs": [],
   "source": [
    "print(\"Glove Word Embeddings of word 'human' \")\n",
    "Glove_model['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760834a",
   "metadata": {
    "papermill": {
     "duration": 0.079887,
     "end_time": "2021-12-07T14:26:50.292131",
     "exception": false,
     "start_time": "2021-12-07T14:26:50.212244",
     "status": "completed"
    },
    "tags": [],
    "id": "0760834a"
   },
   "source": [
    "**Advantages of Word2Vec:**\n",
    "\n",
    "1. The idea is very intuitive, which transforms the unlabeled raw corpus into labeled data (by mapping the target word to its context word), and learns the representation of words in a classification task.\n",
    "2. The data can be fed into the model in an online way and needs little preprocessing, thus requiring little memory.\n",
    "3. the mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words, so that relationships like “king:man as queen: woman” can be inferred by word vectors.\n",
    "3. It is simple for a freshman to understand the principle and do the implementation.\n",
    "\n",
    "**Disadvantages of Word2Vec:**\n",
    "\n",
    "1. The sub-linear relationships are not explicitly defined. There is little theoretical support behind such characteristics.\n",
    "2. The model could be very difficult to train if use the softmax function, since the number of categories is too large (the size of vocabulary). Though approximation algorithms like negative sampling (NEG) and hierarchical softmax (HS) are proposed to address the issue, other problems happen. For example, the word vectors by NEG are not distributed uniformly, they are located within a cone in the vector space hence the vector space is not sufficiently utilized.\n",
    "3. It still provides no way to address the **out of vocabulary(OOV)** problem.\n",
    "\n",
    "**Advantages of Glove:**\n",
    "\n",
    "1. The goal of Glove is very straightforward, i.e., to enforce the word vectors to capture sub-linear relationships in the vector space. Thus, it proves to perform better than Word2vec in the word analogy tasks.\n",
    "2. Glove adds some more practical meaning to word vectors by considering the relationships between word pair and word pair rather than word and word.\n",
    "3. Glove gives lower weight for highly frequent word pairs to prevent the meaningless stop words like “the”, “an” will not dominating the training progress.\n",
    "\n",
    "**Disadvantages of Glove:**\n",
    "\n",
    "1. The model is trained on the co-occurrence matrix of words, which takes a lot of memory for storage. Especially, if you change the hyper-parameters related to the co-occurrence matrix, you have to reconstruct the matrix again, which is very time-consuming.\n",
    "\n",
    "**Both Word2vec and Glove do not solve the problems like:**\n",
    "\n",
    "1. How to learn the representation for out-of-vocabulary words.\n",
    "2. How to separate some opposite word pairs. For example, “good” and “bad” are usually located very close to each other in the vector space, which may limit the performance of word vectors in NLP tasks like sentiment analysis.\n",
    "\n",
    "Content of this cell is taken from [Quora](https://www.quora.com/What-are-the-advantages-and-disadvantages-of-Word2vec-and-GloVe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf12a0",
   "metadata": {
    "papermill": {
     "duration": 0.080012,
     "end_time": "2021-12-07T14:26:50.451727",
     "exception": false,
     "start_time": "2021-12-07T14:26:50.371715",
     "status": "completed"
    },
    "tags": [],
    "id": "a6cf12a0"
   },
   "source": [
    "Despite the ease of using Powerfull Word Embeddings like Word2vec or any such word embedding to do feature extraction from texts, **we don’t have a good way of handling OOV words yet**.\n",
    "\n",
    "What We can do to solve this Problem?\n",
    "\n",
    "1. A simple approach that often works is to exclude those words from the feature extraction process so we don’t have to worry about how to get their representations.\n",
    "\n",
    "2. Another way to deal with the OOV problem for word embeddings is to create vectors that are initialized randomly.\n",
    "\n",
    "3. There are also other approaches that handle the OOV problem by modifying the training process by bringing in characters and other subword-level linguistic components. Let’s look at one such approach now. The key idea is that one can potentially handle the OOV problem by using subword information, such as morphological properties (e.g., prefixes, suffixes, word endings, etc.), **or by using character representations. fastText, from Facebook AI research**, is one of the popular algorithms that follows this approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8224cc0",
   "metadata": {
    "papermill": {
     "duration": 0.079608,
     "end_time": "2021-12-07T14:26:50.611344",
     "exception": false,
     "start_time": "2021-12-07T14:26:50.531736",
     "status": "completed"
    },
    "tags": [],
    "id": "e8224cc0"
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">FastText Word Embeddings</h1><a id = \"10\" ></a>\n",
    "\n",
    "A word can be represented by its constituent character ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n-grams together and views a word’s embedding vector as an aggregation of its constituent character n-grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there’s a word, “gregarious,” that’s not found in the embedding’s word vocabulary. We break it into character n-grams—gre, reg, ega, ….ous—and combine these embeddings of the ngrams to arrive at the embedding of “gregarious.”\n",
    "\n",
    "How FastText Works?\n",
    "\n",
    "FastText is a modified version of word2vec (i.e.. Skip-Gram and CBOW). The only difference between fastText vs word2vec is its pooling strategies (what are the input, output, and dictionary of the model). In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram.\n",
    "\n",
    "**character n-grams** the contiguous sequence of n items from a given sample of a character or word. It may be bigram, trigram, etc.\n",
    "For example character trigram (n = 3) of the word “where” will be:\n",
    "\n",
    "<wh, whe, her, ere, re>\n",
    "\n",
    "In FastText architecture, they have also included the word itself with the character n-gram. That means input data to the model for the word “eating” will be:\n",
    "\n",
    "![](https://amitness.com/images/fasttext-center-word-embedding.png)\n",
    "\n",
    "Now the model I am referring same is word2vec which is a shallow neural network with one hidden layer as discussed above.\n",
    "\n",
    "\n",
    "Now to prepare training data for the (Skip-Gram-based) FastText model, we define “context word” as the word which follows a given word in the text (which will be our “target word”). That means we will be predicting the surrounding word for a given word.\n",
    "\n",
    "Note: FastText word embeddings support both Continuous Bag of Words (CBOW) and Skip-Gram models. I will explain and implement the skip-gram model in the below cell to learn vector representation (FastText word embeddings). Now let’s construct our training examples (like Skip-Gram), scanning through the text with a window will prepare a context word and a target word.\n",
    "\n",
    "Consider the sentence : \n",
    "\n",
    "<div style = \"text-align:center\"><b> i like natural language processing</b></div>\n",
    "\n",
    "![](https://secureservercdn.net/45.40.148.234/um0.ec8.myftpupload.com/wp-content/uploads/2020/10/Picture2.png)\n",
    "\n",
    "For the above example, for context words “i” and “natural” the target word will be “like”. Full training data for FastText word embedding will look like below. By observing the below training data, your confusion of fastText vs word2vec should be clear.\n",
    "\n",
    "\n",
    "Now you know in word2vec (skip-gram) each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram. This training data preparation is the only difference between FastText word embeddings and skip-gram (or CBOW) word embeddings.\n",
    "\n",
    "After training data preparation of FastText, training the word embedding, finding word similarity, etc. are the same as the word2vec model (for our example similar to the skip-gram model).\n",
    "\n",
    "Now let’s see how to implement FastText word embeddings in python using Gensim library.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ],
   "metadata": {
    "id": "k28Z3J14gVq6"
   },
   "id": "k28Z3J14gVq6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cfdf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:26:50.777570Z",
     "iopub.status.busy": "2021-12-07T14:26:50.776795Z",
     "iopub.status.idle": "2021-12-07T14:35:55.608023Z",
     "shell.execute_reply": "2021-12-07T14:35:55.608877Z",
     "shell.execute_reply.started": "2021-12-07T13:21:45.643617Z"
    },
    "papermill": {
     "duration": 544.917726,
     "end_time": "2021-12-07T14:35:55.609232",
     "exception": false,
     "start_time": "2021-12-07T14:26:50.691506",
     "status": "completed"
    },
    "tags": [],
    "id": "1e1cfdf0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0b5eca85-c0f6-45ae-9c4c-65c75c0ac569"
   },
   "outputs": [],
   "source": [
    "print(\"Most similar words to word 'human' : \")\n",
    "fasttext_model.most_similar('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc78fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T14:35:55.933179Z",
     "iopub.status.busy": "2021-12-07T14:35:55.932080Z",
     "iopub.status.idle": "2021-12-07T14:35:55.937079Z",
     "shell.execute_reply": "2021-12-07T14:35:55.937583Z",
     "shell.execute_reply.started": "2021-12-07T13:39:57.957729Z"
    },
    "papermill": {
     "duration": 0.204765,
     "end_time": "2021-12-07T14:35:55.937752",
     "exception": false,
     "start_time": "2021-12-07T14:35:55.732987",
     "status": "completed"
    },
    "tags": [],
    "id": "1bc78fd1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "17e41b71-99b5-4b0c-8ce5-94d1a7f8bca6"
   },
   "outputs": [],
   "source": [
    "print(\"Glove Word Embeddings of word 'human' \")\n",
    "fasttext_model['human']"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def get_embedding_matrix(text):\n",
    "    words = text.split()\n",
    "    embeddings = [fasttext_model[w] for w in words if w in fasttext_model]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(fasttext_model.vector_size)\n",
    "\n",
    "# Convert the training data into embedding matrices\n",
    "X_train = np.array([get_embedding_matrix(x) for x in train_df['text']])\n",
    "y_train = train_df['target']\n",
    "\n",
    "# Convert the testing data into embedding matrices\n",
    "X_test = np.array([get_embedding_matrix(x) for x in test_df['text']])\n",
    "y_test = test_df['target']"
   ],
   "metadata": {
    "id": "36lLHMFzlwkw"
   },
   "id": "36lLHMFzlwkw",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train a logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('Accuracy of logistic regression classifier:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion matrix of logistic regression classifier:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "metadata": {
    "id": "puHAIpAokMsx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a0967a57-1ea3-4b66-f0b3-ef56ca878355"
   },
   "id": "puHAIpAokMsx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(40,),early_stopping=True, max_iter=20)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "print('Accuracy of MLP classifier:', accuracy_score(y_test, y_pred_mlp))\n",
    "print('Confusion matrix of MLP classifier:')\n",
    "print(confusion_matrix(y_test, y_pred_mlp))"
   ],
   "metadata": {
    "id": "dCYlNsDOmIHz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5be493d9-1a50-4f52-e244-5f17b26e8d3e"
   },
   "id": "dCYlNsDOmIHz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "45da4ad8",
   "metadata": {
    "papermill": {
     "duration": 0.08318,
     "end_time": "2021-12-07T14:35:58.185552",
     "exception": false,
     "start_time": "2021-12-07T14:35:58.102372",
     "status": "completed"
    },
    "tags": [],
    "id": "45da4ad8"
   },
   "source": [
    "<b>A potential problem with both approaches is that they do not take the context of words into account. Take, for example, the sentences “cat sat on mat” and “mat sat on cat.” Both receive the same representation in these approaches, but they obviously have very different meanings.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">RNNs</h1><a id = \"10\" ></a>\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for processing sequential data. They operate on a sequence of input data, and they maintain an internal state (i.e., hidden state) that allows them to capture information from earlier parts of the sequence and use it to help process later parts of the sequence.\n",
    "\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "\n",
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">LSTM</h1><a id = \"10\" ></a>\n",
    "Long Short-Term Memory (LSTM) networks are a type of RNN that address the vanishing gradient problem that can occur in standard RNNs. In standard RNNs, the gradients can become very small as they are backpropagated through the network, which can cause the network to have difficulty learning long-term dependencies in the input sequence. LSTMs solve this problem by introducing additional gating mechanisms that allow the network to selectively store and access information in the hidden state.\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">BiLSTMs</h1><a id = \"10\" ></a>\n",
    "Bidirectional LSTMs (BiLSTMs) are a type of LSTM that allow information from both past and future time steps to be incorporated into the hidden state. BiLSTMs achieve this by processing the input sequence in both forward and backward directions, and concatenating the resulting hidden states.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.54.27_PM.png\" width=\"500\">\n",
    "\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ],
   "metadata": {
    "id": "kfODK2sT_LRA"
   },
   "id": "kfODK2sT_LRA"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the input data and labels\n",
    "docs = data.text.tolist()\n",
    "labels = data.target.tolist()\n",
    "\n",
    "# Convert text data to numeric sequences\n",
    "word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
    "for doc in docs:\n",
    "    for word in doc.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "sequences = []\n",
    "for doc in docs:\n",
    "    seq = [word_to_ix[word] for word in doc.split()]\n",
    "    sequences.append(seq)\n",
    "\n",
    "# Pad the sequences to have uniform length\n",
    "max_len = max([len(seq) for seq in sequences])\n",
    "X = np.zeros((len(sequences), max_len))\n",
    "for i, seq in enumerate(sequences):\n",
    "    X[i, :len(seq)] = seq\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "oKrmabAJZoUx"
   },
   "id": "oKrmabAJZoUx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a custom PyTorch Dataset for the text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.X[idx]), torch.LongTensor([self.y[idx]])\n",
    "\n",
    "# Create DataLoaders for the training and test sets\n",
    "batch_size = 2\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "id": "mo5ZpLOuZsn_"
   },
   "id": "mo5ZpLOuZsn_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the RNN model architecture\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True) # nn.LSTM bidirectional=True\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.fc(hidden.squeeze())\n",
    "        return output\n"
   ],
   "metadata": {
    "id": "W-H2uI3CZvz4"
   },
   "id": "W-H2uI3CZvz4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rnn_model = RNN(len(word_to_ix), 32, 32, 3)\n",
    "\n",
    "# Train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters())"
   ],
   "metadata": {
    "id": "jFtXa-lt7RJh"
   },
   "id": "jFtXa-lt7RJh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print('Epoch:', epoch+1, 'Loss:', epoch_loss/len(train_loader))\n"
   ],
   "metadata": {
    "id": "i1VF9ImpZ0oP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d395a799-0af4-4b7f-a7ea-d1eac553f19e"
   },
   "id": "i1VF9ImpZ0oP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = rnn_model(inputs)\n",
    "        y_pred.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ],
   "metadata": {
    "id": "-_6PUV2QZ1uX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c7050f0f-e3a4-4a5b-b44e-85f95189adcd"
   },
   "id": "-_6PUV2QZ1uX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Task 1: Define a BiLSTM model architecture which uses pretrained word embeddings passed as parameters.</h1>"
   ],
   "metadata": {
    "id": "PfDnOO6XxiM5"
   },
   "id": "PfDnOO6XxiM5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the BiLSTM model architecture\n",
    "class BiLSTM(nn.Module):\n",
    "    # Write your code here\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_embeddings):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=0)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.bilstm(x)\n",
    "        output = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return output"
   ],
   "metadata": {
    "id": "2mfDbPc3zawC"
   },
   "id": "2mfDbPc3zawC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Dictonary of embeddings of words in vocabulary\n",
    "pretrained_embeddings = np.zeros((len(word_to_ix), 25))\n",
    "for word, idx in word_to_ix.items():\n",
    "    if word in Glove_model.vocab:\n",
    "        pretrained_embeddings[idx] = Glove_model[word]\n",
    "    else:\n",
    "        pretrained_embeddings[idx] = 1"
   ],
   "metadata": {
    "id": "g7iR3NqTzefE"
   },
   "id": "g7iR3NqTzefE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bilstm_model = BiLSTM(len(word_to_ix), 25, 32, 3, torch.FloatTensor(pretrained_embeddings))"
   ],
   "metadata": {
    "id": "X1SkgwTlzfCv"
   },
   "id": "X1SkgwTlzfCv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bilstm_model.parameters())\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bilstm_model(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print('Epoch:', epoch+1, 'Loss:', epoch_loss/len(train_loader))"
   ],
   "metadata": {
    "id": "6Bfyed7z2mu-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e08e4bf1-4421-4e3f-fa60-cf21feb5717c"
   },
   "id": "6Bfyed7z2mu-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = bilstm_model(inputs)\n",
    "        y_pred.extend(torch.argmax(outputs, dim=1).tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ],
   "metadata": {
    "id": "4XT_6sjJ-HfI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a390d260-0581-4cc2-ab3b-b3a26eaf8ede"
   },
   "id": "4XT_6sjJ-HfI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>One of the main disadvantages of LSTMs is that they still require sequential processing of the input sequence, which can be computationally expensive for long sequences. Additionally, LSTMs may have difficulty modeling very long-term dependencies or capturing complex relationships between distant parts of the sequence.\n",
    "\n",
    "Transformers are a type of neural network architecture that were introduced in 2017. Unlike LSTMs, Transformers use a self-attention mechanism to directly model the relationships between all elements in the input sequence, rather than relying on sequential processing. This allows Transformers to efficiently capture long-range dependencies and complex relationships between elements in the sequence, without the computational overhead of processing the sequence sequentially.</b>"
   ],
   "metadata": {
    "id": "-F2cQibkCDq3"
   },
   "id": "-F2cQibkCDq3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Task 2: Visualize some of the classification examples using LIME. You will find the relevent codes in following links. Adapt the code to meet your needs.</h1>\n",
    "\n",
    "[pytorch rnn for text classification tasks](https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks)\n",
    "\n",
    "[lime repo](https://github.com/marcotcr/lime)"
   ],
   "metadata": {
    "id": "eHcqp3Z57M0i"
   },
   "id": "eHcqp3Z57M0i"
  },
  {
   "cell_type": "code",
   "source": [
    "# Write your code here\n",
    "\n",
    "!pip install lime -q"
   ],
   "metadata": {
    "id": "8iFzCLTi8PYK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d71e401-4e6f-45da-e22b-f31aaadf51e8"
   },
   "id": "8iFzCLTi8PYK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from lime import lime_text\n",
    "\n",
    "explainer = lime_text.LimeTextExplainer(class_names=['neutral', 'negative', 'positive'], verbose=True)"
   ],
   "metadata": {
    "id": "0Fz4giwgHo0f"
   },
   "id": "0Fz4giwgHo0f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def process_docs(docs):\n",
    "    word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    \n",
    "    sequences = []\n",
    "    for doc in docs:\n",
    "        seq = [word_to_ix[word] for word in doc.split()]\n",
    "        sequences.append(seq)\n",
    "\n",
    "    # Pad the sequences to have uniform length\n",
    "    max_len = max([len(seq) for seq in sequences])\n",
    "    X = np.zeros((len(sequences), max_len))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        X[i, :len(seq)] = seq\n",
    "    \n",
    "    X = torch.LongTensor(X)\n",
    "    return X"
   ],
   "metadata": {
    "id": "w7JdWp6PSbll"
   },
   "id": "w7JdWp6PSbll",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "idx = 9\n",
    "x, y = data['text'][idx], data['target'][idx]"
   ],
   "metadata": {
    "id": "84NGfZT7SkBR"
   },
   "id": "84NGfZT7SkBR",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_predictions(docs):\n",
    "    X = process_docs(docs)\n",
    "    outputs = rnn_model(X)\n",
    "    preds = outputs.detach().numpy()\n",
    "    return preds\n",
    "\n",
    "explanation = explainer.explain_instance(x, classifier_fn=make_predictions, labels=[y])\n",
    "explanation.show_in_notebook()"
   ],
   "metadata": {
    "id": "bPiPZNTsHqj4",
    "outputId": "9fdfc3a7-20ba-43ca-dd70-fcc363eda765",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    }
   },
   "id": "bPiPZNTsHqj4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_predictions(docs):\n",
    "    X = process_docs(docs)\n",
    "    outputs = bilstm_model(X)\n",
    "    preds = outputs.detach().numpy()\n",
    "    return preds\n",
    "\n",
    "explanation = explainer.explain_instance(x, classifier_fn=make_predictions, labels=[y])\n",
    "explanation.show_in_notebook()"
   ],
   "metadata": {
    "id": "4lhxP1TVHshL",
    "outputId": "b4701ba5-fc3d-4080-cb47-180d53795a58",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    }
   },
   "id": "4lhxP1TVHshL",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 853.142536,
   "end_time": "2021-12-07T14:36:10.246332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-07T14:21:57.103796",
   "version": "2.3.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
