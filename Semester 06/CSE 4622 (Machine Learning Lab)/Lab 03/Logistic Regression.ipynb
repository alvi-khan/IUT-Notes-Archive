{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of version2 Logistic_Regression_with_Breast_Cancer_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration"
      ],
      "metadata": {
        "id": "VIe823_ui0fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Data"
      ],
      "metadata": {
        "id": "s-X7rHOLi5K-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9WE55jG7Jc-"
      },
      "outputs": [],
      "source": [
        "!gdown --id -q 11Cxt5om7r7xuWEhMWlmljX-PHag9adZq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading the Data\n",
        "data = pd.read_csv('/content/data.csv')\n",
        "data.head(20)"
      ],
      "metadata": {
        "id": "VGCSI3nK8rMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "Y_ibXW4C81ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the health of the data itself. This function `info()` in the pandas library is very helpful to understand the basic properties of the data itself. If there is any missing values in the dataset can be known right from here so that they can be taken care of before fitting into a model for training and testing."
      ],
      "metadata": {
        "id": "tdBW9oWI9THG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "_jfSJWnA9Ne4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are 3 things that take my attention.**\n",
        "\n",
        "\n",
        "1.   There is an `id` that cannot be used for classificaiton\n",
        "2.   `Diagnosis` column in the data is our class label\n",
        "3.   `Unnamed: 32` feature includes `NaN` so we do not need it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HjOY15HG91SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing"
      ],
      "metadata": {
        "id": "fYyn120XjAED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = data.columns       \n",
        "print(col)\n",
        "\n",
        "\n",
        "data.drop(['Unnamed: 32',\"id\"], axis=1, inplace=True)\n",
        "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
        "y_data = data.diagnosis.values\n",
        "x_data = data.drop(['diagnosis'], axis=1)\n",
        "x_data"
      ],
      "metadata": {
        "id": "GTIUc3w99O5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_data"
      ],
      "metadata": {
        "id": "vru4S1lE-hiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now we have the features but **what do they mean**, or actually **how much do we need to know about these features**. The answer is that we do not need to know the meaning of these features. However, to imagine in our mind, we should know something like variance, standard deviation, number of the sample (count), or max-min values. This type of information helps to understand what is going on in data. For example, this question appears in my mind that the `area_mean` feature's max value is `2500` and `smoothness_mean` features' max `0.16340`. Therefore **do we need standardization or normalization before visualization, feature selection, feature extraction, or classification**? The answer is yes and no not surprising ha :) Anyway, let us go step by step and start with visualization."
      ],
      "metadata": {
        "id": "nYbDRo8_-vxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "EcOAjwZZjDEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Normalization refers to rescaling real-valued numeric attributes into a `0 to 1` range. Data normalization is used in machine learning to make model training less sensitive to the scale of features.\n",
        "\n",
        "You can either implement the conversion process with basic python or use `MinMaxScaler()` function from the `sklearn` library."
      ],
      "metadata": {
        "id": "0tTS_GMEAFx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using transformer from sklearn library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalar = MinMaxScaler()\n",
        "output = scalar.fit_transform(x_data)\n",
        "\n",
        "# Manual Implementation of the normalization process\n",
        "X_data = (x_data -np.min(x_data))/ (np.max(x_data)-np.min(x_data)).values"
      ],
      "metadata": {
        "id": "FFyNfjuG-iGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data"
      ],
      "metadata": {
        "id": "KD0pJrjdAjQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Basics: Logistic Regression and Regularization"
      ],
      "metadata": {
        "id": "sdEkdTGxjV-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is one of the most common machine learning algorithms used for classification. It is a statistical model that uses a logistic function to model a binary dependent variable. In essence, it predicts the probability of an observation belonging to a certain class or label. For instance, is this a cat photo or a dog photo?\n",
        "\n",
        "Ordinary Least Squares linear regression is powerful and versatile right out of the box, but there are certain circumstances where it fails.\n",
        "\n",
        "1. It is, expressly, a ‘regression’ framework, which makes it hard to apply as a classifier.\n",
        "2. Unlike, say, a decision tree, linear regression models don’t perform their implicit feature selection, meaning they are prone to overfit if too many features are included.\n",
        "\n",
        "Luckily, there are some extensions to the linear model that allow us to overcome these issues. Logistic regression turns the linear regression framework into a classifier and various types of `regularization` of which the `Ridge` and `Lasso` methods are most common, help avoid overfit in feature-rich instances."
      ],
      "metadata": {
        "id": "6eZMK0ZsBA1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "\n",
        "We want our model to predict the probability of an observation belonging to a certain class or label. As such, we want a hypothesis $h$ that satisfies the following condition $0 <= h(x) <= 1$ , where $x$ is an observation.\n",
        "\n",
        "We define $h(x) = g(w^T * x)$ , where $g$ is a sigmoid function and $w$ are the trainable parameters or `weights`. As such, we have:\n",
        "$$h(x) = \\frac{1}{1+e^{-w^Tx}}$$"
      ],
      "metadata": {
        "id": "-FDlt8jDjvmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The cost for an observation \n",
        "Now that we can predict the probability for an observation, we want the result to have the minimum error. If the class label is $y$, the cost (error) associated with an observation $x$ is given by:\n",
        "\n",
        "![](https://miro.medium.com/max/525/1*vSGnYVz6I7sAObKuxuFAoQ.gif)"
      ],
      "metadata": {
        "id": "hR5vfrqkj4Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function\n",
        "Thus, the total cost for all the $m$ observations in a dataset is:\n",
        "![](https://miro.medium.com/max/368/0*vZnp94vCoN0vMDAj)\n",
        "\n",
        "We can rewrite the cost function J as:\n",
        "![](https://miro.medium.com/max/691/0*o57ug0iMGDJVI1qo)\n",
        "\n",
        "The objective of logistic regression is to find params `w` so that `J` is minimum. How can we do that?? We will use the gradient descent algorithm to update each of the weights gradually to minimize the cost `J`. \n",
        "\n",
        "We will update each of the params wᵢ using the following template:\n",
        "![](https://miro.medium.com/max/875/0*Q6ssvXABrvHUZrfy)\n",
        "![](https://miro.medium.com/max/496/0*7uVvuW-ZGauNWH_V)\n",
        "\n",
        "The above step will help us find a set of params wᵢ, which will then help us to come up with $h(x)$ to solve our binary classification task.\n",
        "But there is also an undesirable outcome associated with the above gradient descent steps. In an attempt to find the best $h(x)$, the following things happen:\n",
        "\n",
        "**CASE I: For class label = 0**: $h(x)$ will try to produce results as close 0 as possible. As such, $w^T.x$ will be as small as possible\n",
        "=> Wi will tend to -infinity\n",
        "\n",
        "**CASE II: For class label = 1**: $h(x)$ will try to produce results as close 1 as possible. As such, $w^T.x$ will be as large as possible\n",
        "=> Wi will tend to +infinity"
      ],
      "metadata": {
        "id": "3WZIiahRj7y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization\n",
        "Regularization is a technique to solve the problem of overfitting in a machine learning algorithm by penalizing the cost function. It does so by using an additional penalty term in the cost function.\n",
        "There are two types of regularization techniques:\n",
        "1. Lasso or L1 Regularization\n",
        "2. Ridge or L2 Regularization (We will implement here)\n",
        "So, how can L2 Regularization help to prevent overfitting? Let’s first look at our new cost function:\n",
        "\n",
        "![](https://miro.medium.com/max/628/0*Nc_ocecF0dHpUutK)\n",
        "\n",
        "\n",
        "The regularization term will heavily penalize large $w_i$. The effect will be less on smaller $w_i$’s. As such, the growth of $w$ is controlled. The $h(x)$ we obtain with these controlled params $w$ will be more generalizable.\n",
        "\n",
        "**NOTE:** $λ$ is a hyper-parameter value. We have to find it using cross-validation. \n",
        "* Larger value $λ$ of will make $w_i$ shrink closer to $0$, which might lead to underfitting. \n",
        "* $λ = 0$, will have no regulariztion effect. \n",
        "\n",
        "When choosing $λ$, we have to take proper care of bias vs variance trade-off."
      ],
      "metadata": {
        "id": "nagHb5TbB0my"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Code"
      ],
      "metadata": {
        "id": "1qlxcX5AkI9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLogisticRegression(object):\n",
        "    \"\"\"\n",
        "    Logistic Regression Classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    learning_rate : int or float, default=0.1\n",
        "        The tuning parameter for the optimization algorithm (here, Gradient Descent) \n",
        "        that determines the step size at each iteration while moving toward a minimum \n",
        "        of the cost function.\n",
        "    max_iter : int, default=100\n",
        "        Maximum number of iterations taken for the optimization algorithm to converge\n",
        "    \n",
        "    penalty : None or 'l2', default='l2'.\n",
        "        Option to perform L2 regularization.\n",
        "    C : float, default=0.1\n",
        "        Inverse of regularization strength; must be a positive float. \n",
        "        Smaller values specify stronger regularization. \n",
        "    tolerance : float, optional, default=1e-4\n",
        "        Value indicating the weight change between epochs in which\n",
        "        gradient descent should terminated. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.1, max_iter=100, regularization='l2', lambda_ = 10 , tolerance = 1e-4):\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iter       = max_iter\n",
        "        self.regularization = regularization\n",
        "        self.lambda_        = lambda_\n",
        "        self.tolerance      = tolerance\n",
        "        self.loss_log       = []\n",
        "    \n",
        "    def fit(self, X, y, verbose = False):\n",
        "        \"\"\"\n",
        "        Fit the model according to the given training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            Training vector, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target vector relative to X.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        self.theta = np.random.rand(X.shape[1] + 1)\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        self.loss_log = []\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            Z = np.matmul(X,  self.theta)\n",
        "            y_hat = self.__sigmoid(Z)\n",
        "            \n",
        "            errors = y_hat - y\n",
        "\n",
        "            N = X.shape[1] \n",
        "            \n",
        "            cost = (-1.0/N) * np.sum( y*np.log(y_hat) + (1.0 - y)*np.log(1.0-y_hat))\n",
        "            self.loss_log.append(cost)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f'Iteration {iteration} Loss: {cost}')\n",
        "\n",
        "            if self.regularization is not None:\n",
        "                delta_grad = (1./N) *(np.matmul(errors.T, X)+ self.lambda_ * self.theta)\n",
        "            else:\n",
        "                delta_grad = (1./N) *(np.matmul(errors.T, X))\n",
        "                \n",
        "            self.theta -= self.learning_rate * delta_grad\n",
        "\n",
        "#             if np.all(abs(delta_grad) >= self.tolerance):\n",
        "#                 self.theta -= self.learning_rate * delta_grad\n",
        "#             else:\n",
        "#                 break\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Probability estimates for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Vector to be scored, where `n_samples` is the number of samples and\n",
        "            `n_features` is the number of features.\n",
        "        Returns\n",
        "        -------\n",
        "        probs : array-like of shape (n_samples,)\n",
        "            Returns the probability of each sample.\n",
        "        \"\"\"\n",
        "        return self.__sigmoid((X @ self.theta[1:]) + self.theta[0])\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array_like or sparse matrix, shape (n_samples, n_features)\n",
        "            Samples.\n",
        "        Returns\n",
        "        -------\n",
        "        labels : array, shape [n_samples]\n",
        "            Predicted class label per sample.\n",
        "        \"\"\"\n",
        "        return np.round(self.predict_proba(X))\n",
        "        \n",
        "    def __sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        The sigmoid function.\n",
        "        Parameters\n",
        "        ------------\n",
        "        z : float\n",
        "            linear combinations of weights and sample features\n",
        "            z = w_0 + w_1*x_1 + ... + w_n*x_n\n",
        "        Returns\n",
        "        ---------\n",
        "        Value of logistic function at z\n",
        "        \"\"\"\n",
        "        return (1.0 / (1.0 + np.exp(-z)))\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Get method for models coeffients and intercept.\n",
        "        Returns\n",
        "        -------\n",
        "        params : dict\n",
        "        \"\"\"\n",
        "        try:\n",
        "            params = dict()\n",
        "            params['intercept'] = self.theta[0]\n",
        "            params['coef'] = self.theta[1:]\n",
        "            return params\n",
        "        except:\n",
        "            raise Exception('Fit the model first!')"
      ],
      "metadata": {
        "id": "NkuVzToYA9wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Data Validation"
      ],
      "metadata": {
        "id": "apsHs0SUkQYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us split the whole data into two portion. We take 80% data in the train set and then put rest of the data into the test set to check the performance of the trained model. "
      ],
      "metadata": {
        "id": "3hqNbBtyC2Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=42)\n",
        "\n",
        "# Train and Test Data Summary\n",
        "import plotly.graph_objects as go\n",
        "split = ['Train','Test']\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(x=split, y=[np.sum(y_train), np.sum(y_test)],#                base=[-500,-600],\n",
        "                    marker_color='crimson',\n",
        "                    name='Malignant'))\n",
        "fig.add_trace(go.Bar(x=split, \n",
        "                     y=[len(y_train)- np.sum(y_train), len(y_test) - np.sum(y_test)],\n",
        "                    base=0,\n",
        "                    marker_color='lightgreen',\n",
        "                    name='Benign'                ))\n",
        "fig.update_layout(width = 800, height = 400)\n",
        "fig.update_layout(title = 'Count of Samples in Train and Test Split', title_x = 0.5, xaxis_title = \"Category\", yaxis_title = 'Sample Count')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "cGBONkO0Cy89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Logistic Regression"
      ],
      "metadata": {
        "id": "OTIhr7KmkTk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the logistic regression with the training data for a maximum interation of 200. Other parameters are kept default. Feel free to fiddle around the other parameters to understand more of them. "
      ],
      "metadata": {
        "id": "ot2b9mciDAJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ITER = 400\n",
        "LR_RATE = 1e-2\n",
        "\n",
        "clf_no_reg = CustomLogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE,  lambda_ = 200, regularization= None)\n",
        "clf_no_reg.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "clf_reg_10 = CustomLogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 10, regularization= 'l2')\n",
        "clf_reg_10.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "\n",
        "clf_reg_20 = CustomLogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 20, regularization= 'l2')\n",
        "clf_reg_20.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "clf_reg_40 = CustomLogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 40, regularization= 'l2')\n",
        "clf_reg_40.fit(X_train, y_train, verbose = False)"
      ],
      "metadata": {
        "id": "_EEAdHzbDFUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training Loss over Time"
      ],
      "metadata": {
        "id": "4s9F2_5hDKsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "y = clf_no_reg.loss_log \n",
        "\n",
        "# fig = go.Figure(data=go.Scatter(x= np.arange(start =1, stop = len(y)), \n",
        "#                                 y=y,\n",
        "#                                 mode = 'lines+markers'))\n",
        "\n",
        "\n",
        "# Create traces\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_no_reg.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='No Regularization'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_10.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='Regulrization W=10'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_20.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='Regulrization W=20'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_40.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='Regulrization W=40'))\n",
        "\n",
        "\n",
        "fig.update_layout(title = \"Error Plot over Iterations\", title_x = 0.5,\n",
        "                  xaxis_title = 'Iteration',\n",
        "                  yaxis_title = 'Log Loss',\n",
        "                  width = 800,\n",
        "                  height = 500)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2vgX40VjDOzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Calculation"
      ],
      "metadata": {
        "id": "sj7r2frVDiM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_no_reg.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_10.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_20.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_40.predict(X_test))*100}%')"
      ],
      "metadata": {
        "id": "z0DOZ2VQD3iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Confusion Matrix"
      ],
      "metadata": {
        "id": "N8MK4yf1D8BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, clf_no_reg.predict(X_test)))\n",
        "sns.heatmap(confusion_matrix(y_test, clf_no_reg.predict(X_test)), annot=True)"
      ],
      "metadata": {
        "id": "COJpVSpjD5iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "IjikyW-6kgDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply logistic regression on MNIST dataset. \n",
        "The MNIST database of handwritten digits, available from this [page](http://yann.lecun.com/exdb/mnist/), has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
        "\n",
        "You can use [scikit-learn's `LogisticRegression` class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), or, you can use the class we just have created. The images in MNIST dataset are of shape (28 x 28) pixels. So, there are 784 pixels in total. You can consider each pixel as a feature. In summary, a sample from your training or test set will be of shape (1 x 784). "
      ],
      "metadata": {
        "id": "ZOVXDsGugEa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "6S81TGIugvMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
      ],
      "metadata": {
        "id": "6uGuohg3gzhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Plot one image from each class e.g., (0, 1, 2, 3, ...)\n",
        "\n",
        "Hint: Create a function `view_image(img)` that will take a numpy array of shape (1, 784) as input and reshape it to (28,28) numpy array. Then it will plot this numpy array in grayscale using `matplotlib.pyplot`."
      ],
      "metadata": {
        "id": "JTZ0cN7ri_mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "def view_image(img):\n",
        "  plt.imshow(img.reshape((28, 28)))\n",
        "  plt.gray()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "KvlZivdKi0rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Print one sample and decide whether you need to normalize the images. If yes, then do so."
      ],
      "metadata": {
        "id": "8uNKn4kIjiH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "import numpy as np\n",
        "\n",
        "values, indices = np.unique(y, return_index = True)\n",
        "for index in indices:\n",
        "  view_image(X[index])\n",
        "\n"
      ],
      "metadata": {
        "id": "mpE4_lNZis5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "YSLfRdzdzRY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Split your dataset into training and test set. Use `random_state = 7`. 80% of your whole data should be in training set."
      ],
      "metadata": {
        "id": "9nsWXqTijxN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7)"
      ],
      "metadata": {
        "id": "qpZqWkcwi_EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train the model for maximum 100 iterations without any regularization. Evaluate your trained model on test set. \n",
        "\n"
      ],
      "metadata": {
        "id": "V9El0QKfn7gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "\n",
        "clf_no_reg = LogisticRegression(penalty='none', max_iter=100, solver='saga', tol=0.1)\n",
        "#clf_no_reg = CustomLogisticRegression(max_iter = 100, learning_rate= 1e-2,  lambda_ = 200, regularization= None)\n",
        "clf_no_reg.fit(X_train, y_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_no_reg.predict(X_test))*100}%')"
      ],
      "metadata": {
        "id": "3InmC1jXivpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.   Show the effect of different `penalty/regularization` ('l1', 'l2') method."
      ],
      "metadata": {
        "id": "3TtvYOg4o87s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_reg_l1 = LogisticRegression(penalty='l1', max_iter=100, solver='saga', tol=0.1)\n",
        "clf_reg_l2 = LogisticRegression(penalty='l2', max_iter=100, solver='saga', tol=0.1)\n",
        "\n",
        "clf_reg_l1.fit(X_train, y_train)\n",
        "clf_reg_l2.fit(X_train, y_train)\n",
        "\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_no_reg.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_l1.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_l2.predict(X_test))*100}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "bi7vO_NcpDjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Show the effect of different `regularization strength` (1/C while using scikit-learn)."
      ],
      "metadata": {
        "id": "cu-YCTvcpHWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "\n",
        "reg_strenghts = [1.0, 10.0, 100.0]\n",
        "\n",
        "for c in reg_strenghts:\n",
        "  clf_reg_l2 = LogisticRegression(C=1/c, penalty='l2', max_iter=100, solver='saga', tol=0.1)\n",
        "  clf_reg_l2.fit(X_train, y_train)\n",
        "  print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_l2.predict(X_test))*100}%')"
      ],
      "metadata": {
        "id": "xk9mlNAWpMpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Plot `5` misclassified samples, their `actual label`, and `predicted label` using the output from your best performing model."
      ],
      "metadata": {
        "id": "uo0lNE6QqArD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "predictions = clf_reg_l2.predict(X_test)\n",
        "misclassifications = np.where(y_test != predictions)\n",
        "print(len(misclassifications[0]))\n",
        "print(len(y_test))\n",
        "items = 0\n",
        "for i in misclassifications[0]:\n",
        "  items = items + 1\n",
        "  print(\"Actual Label: \" + y_test[i])\n",
        "  print(\"Predicted Label: \" + predictions[i])\n",
        "  view_image(X_test[i])\n",
        "  if(items == 5):\n",
        "    break"
      ],
      "metadata": {
        "id": "Sen_rfJ7qIyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Plot the confusion matrix of the test set of best performing setup. "
      ],
      "metadata": {
        "id": "y6dcWt5dFeXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, clf_reg_l2.predict(X_test)))\n",
        "sns.heatmap(confusion_matrix(y_test, clf_reg_l2.predict(X_test)), annot=True)"
      ],
      "metadata": {
        "id": "ivybcHSnFuOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OUlNLiRU5ott"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}